{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from dotenv import load_dotenv\n",
                "import json\n",
                "import sqlite3\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "import numpy as np\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from openai import OpenAI\n",
                "import os\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "import torch\n",
                "\n",
                "load_dotenv()\n",
                "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
                "client = OpenAI()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "movies_df = pd.read_csv(\"https://raw.githubusercontent.com/4GeeksAcademy/k-nearest-neighbors-project-tutorial/main/tmdb_5000_movies.csv\")\n",
                "credits_df = pd.read_csv(\"https://raw.githubusercontent.com/4GeeksAcademy/k-nearest-neighbors-project-tutorial/main/tmdb_5000_credits.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(4803, 20)"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "movies_df.head(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "       id                                     title  \\\n",
                        "0   19995                                    Avatar   \n",
                        "1     285  Pirates of the Caribbean: At World's End   \n",
                        "2  206647                                   Spectre   \n",
                        "3   49026                     The Dark Knight Rises   \n",
                        "4   49529                               John Carter   \n",
                        "\n",
                        "                                            overview  \\\n",
                        "0  In the 22nd century, a paraplegic Marine is di...   \n",
                        "1  Captain Barbossa, long believed to be dead, ha...   \n",
                        "2  A cryptic message from Bond’s past sends him o...   \n",
                        "3  Following the death of District Attorney Harve...   \n",
                        "4  John Carter is a war-weary, former military ca...   \n",
                        "\n",
                        "                                              genres  \\\n",
                        "0  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
                        "1  [{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"...   \n",
                        "2  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
                        "3  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 80, \"nam...   \n",
                        "4  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
                        "\n",
                        "                                            keywords  \\\n",
                        "0  [{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...   \n",
                        "1  [{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na...   \n",
                        "2  [{\"id\": 470, \"name\": \"spy\"}, {\"id\": 818, \"name...   \n",
                        "3  [{\"id\": 849, \"name\": \"dc comics\"}, {\"id\": 853,...   \n",
                        "4  [{\"id\": 818, \"name\": \"based on novel\"}, {\"id\":...   \n",
                        "\n",
                        "                                                cast  \\\n",
                        "0  [{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...   \n",
                        "1  [{\"cast_id\": 4, \"character\": \"Captain Jack Spa...   \n",
                        "2  [{\"cast_id\": 1, \"character\": \"James Bond\", \"cr...   \n",
                        "3  [{\"cast_id\": 2, \"character\": \"Bruce Wayne / Ba...   \n",
                        "4  [{\"cast_id\": 5, \"character\": \"John Carter\", \"c...   \n",
                        "\n",
                        "                                                crew  \n",
                        "0  [{\"credit_id\": \"52fe48009251416c750aca23\", \"de...  \n",
                        "1  [{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de...  \n",
                        "2  [{\"credit_id\": \"54805967c3a36829b5002c41\", \"de...  \n",
                        "3  [{\"credit_id\": \"52fe4781c3a36847f81398c3\", \"de...  \n",
                        "4  [{\"credit_id\": \"52fe479ac3a36847f813eaa3\", \"de...  \n"
                    ]
                }
            ],
            "source": [
                "# Create a connection to a new SQLite database\n",
                "conn = sqlite3.connect('movies.db')\n",
                "\n",
                "# Store the DataFrames in SQL tables\n",
                "movies_df.to_sql('movies', conn, if_exists='replace', index=False)\n",
                "credits_df.to_sql('credits', conn, if_exists='replace', index=False)\n",
                "\n",
                "# Join the tables using SQL\n",
                "query = '''\n",
                "SELECT \n",
                "    movies.id,\n",
                "    movies.title,\n",
                "    movies.overview,\n",
                "    movies.genres,\n",
                "    movies.keywords,\n",
                "    credits.cast,\n",
                "    credits.crew\n",
                "FROM movies\n",
                "JOIN credits\n",
                "ON movies.title = credits.title\n",
                "'''\n",
                "\n",
                "# Create a unified DataFrame\n",
                "unified_df = pd.read_sql(query, conn)\n",
                "\n",
                "# Display the first few rows to ensure the join is correct\n",
                "print(unified_df.head())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "       id                                     title  \\\n",
                        "0   19995                                    Avatar   \n",
                        "1     285  Pirates of the Caribbean: At World's End   \n",
                        "2  206647                                   Spectre   \n",
                        "3   49026                     The Dark Knight Rises   \n",
                        "4   49529                               John Carter   \n",
                        "\n",
                        "                                                tags  \n",
                        "0  In the 22nd century, a paraplegic Marine is di...  \n",
                        "1  Captain Barbossa, long believed to be dead, ha...  \n",
                        "2  A cryptic message from Bond’s past sends him o...  \n",
                        "3  Following the death of District Attorney Harve...  \n",
                        "4  John Carter is a war-weary, former military ca...  \n"
                    ]
                }
            ],
            "source": [
                "def parse_json(data, key):\n",
                "    \"\"\" Helper function to parse JSON and extract a specific key \"\"\"\n",
                "    try:\n",
                "        return ' '.join([item[key] for item in json.loads(data)])\n",
                "    except:\n",
                "        return ''\n",
                "\n",
                "def parse_cast(data):\n",
                "    \"\"\" Extract the first three cast members \"\"\"\n",
                "    try:\n",
                "        cast_list = json.loads(data)\n",
                "        return ' '.join([member['name'] for member in cast_list[:3]])\n",
                "    except:\n",
                "        return ''\n",
                "\n",
                "def parse_crew(data):\n",
                "    \"\"\" Extract the director's name \"\"\"\n",
                "    try:\n",
                "        crew_list = json.loads(data)\n",
                "        for member in crew_list:\n",
                "            if member['job'] == 'Director':\n",
                "                return member['name']\n",
                "        return ''\n",
                "    except:\n",
                "        return ''\n",
                "\n",
                "# Parse JSON columns\n",
                "unified_df['genres'] = unified_df['genres'].apply(parse_json, key='name')\n",
                "unified_df['keywords'] = unified_df['keywords'].apply(parse_json, key='name')\n",
                "unified_df['cast'] = unified_df['cast'].apply(parse_cast)\n",
                "unified_df['crew'] = unified_df['crew'].apply(parse_crew)\n",
                "\n",
                "# Fill None values in overview with empty string\n",
                "unified_df['overview'] = unified_df['overview'].fillna('')\n",
                "\n",
                "# Convert overview to a list of words, if it's not already a list\n",
                "def convert_overview(x):\n",
                "    if isinstance(x, list):\n",
                "        return x\n",
                "    return x.split()\n",
                "\n",
                "unified_df['overview'] = unified_df['overview'].apply(convert_overview)\n",
                "\n",
                "# Remove spaces in specified columns\n",
                "for column in ['genres', 'keywords', 'cast', 'crew']:\n",
                "    unified_df[column] = unified_df[column].str.replace(' ', '')\n",
                "\n",
                "# Combine all columns into a single 'tags' column\n",
                "unified_df['tags'] = unified_df.apply(lambda x: ' '.join(x['overview']) + ' ' + x['genres'] + ' ' + x['keywords'] + ' ' + x['cast'] + ' ' + x['crew'], axis=1)\n",
                "\n",
                "# Display the first few rows of the transformed DataFrame\n",
                "print(unified_df[['id', 'title', 'tags']].head())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Apollo 18\n",
                        "Beowulf\n",
                        "Tears of the Sun\n",
                        "The American\n",
                        "The Book of Life\n"
                    ]
                }
            ],
            "source": [
                "# Vectorize the 'tags' column\n",
                "cv = CountVectorizer(max_features=5000, stop_words='english')\n",
                "vectors = cv.fit_transform(unified_df['tags']).toarray()\n",
                "\n",
                "# Compute cosine similarity\n",
                "similarity = cosine_similarity(vectors)\n",
                "\n",
                "# Recommendation function\n",
                "def recommend_cv(movie):\n",
                "    movie_index = unified_df[unified_df['title'] == movie].index[0]\n",
                "    distances = similarity[movie_index]\n",
                "    movie_list = sorted(list(enumerate(distances)), reverse=True, key=lambda x: x[1])[1:6]\n",
                "    \n",
                "    for i in movie_list:\n",
                "        print(unified_df.iloc[i[0]].title)\n",
                "\n",
                "# Example usage\n",
                "recommend_cv(\"Avatar\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mission: Impossible II\n",
                        "Mission: Impossible III\n",
                        "Zero Dark Thirty\n",
                        "Compadres\n",
                        "Bad Company\n"
                    ]
                }
            ],
            "source": [
                "# Create labels for KNN\n",
                "labels = np.arange(vectors.shape[0])\n",
                "\n",
                "# Initialize the KNN classifier\n",
                "knn = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n",
                "knn.fit(vectors, labels)\n",
                "\n",
                "\n",
                "def recommend_knn(movie):\n",
                "    movie_index = unified_df[unified_df['title'] == movie].index[0]\n",
                "    movie_vector = vectors[movie_index].reshape(1, -1)\n",
                "    distances, indices = knn.kneighbors(movie_vector, n_neighbors=6)\n",
                "    \n",
                "    recommended_indices = indices.flatten()[1:]  # Skip the first one since it's the same movie\n",
                "    \n",
                "    for i in recommended_indices:\n",
                "        print(unified_df.iloc[i].title)\n",
                "\n",
                "\n",
                "recommend_knn(\"Mission: Impossible\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "unified_df[\"combined\"] = (\n",
                "    \"title: \" + unified_df.title.str.strip() + \"; tags: \" + unified_df.tags.str.strip()\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
                "   text = text.replace(\"\\n\", \" \")\n",
                "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "ename": "RateLimitError",
                    "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m unified_df[\u001b[39m'\u001b[39m\u001b[39mada_embedding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m unified_df\u001b[39m.\u001b[39;49mcombined\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: get_embedding(x, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtext-embedding-3-small\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      2\u001b[0m unified_df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39m../data/processed/embedded_movies.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1138\u001b[0m             values,\n\u001b[0;32m   1139\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1141\u001b[0m         )\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
                        "Cell \u001b[1;32mIn[10], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m unified_df[\u001b[39m'\u001b[39m\u001b[39mada_embedding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m unified_df\u001b[39m.\u001b[39mcombined\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: get_embedding(x, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtext-embedding-3-small\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      2\u001b[0m unified_df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39m../data/processed/embedded_movies.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
                        "Cell \u001b[1;32mIn[9], line 3\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embedding\u001b[39m(text, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-embedding-3-small\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m    text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m    \u001b[39mreturn\u001b[39;00m client\u001b[39m.\u001b[39;49membeddings\u001b[39m.\u001b[39;49mcreate(\u001b[39minput\u001b[39;49m \u001b[39m=\u001b[39;49m [text], model\u001b[39m=\u001b[39;49mmodel)\u001b[39m.\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39membedding\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\openai\\resources\\embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    108\u001b[0m         embedding\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfrombuffer(  \u001b[39m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    109\u001b[0m             base64\u001b[39m.\u001b[39mb64decode(data), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n\u001b[1;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m    115\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/embeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    116\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(params, embedding_create_params\u001b[39m.\u001b[39;49mEmbeddingCreateParams),\n\u001b[0;32m    117\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    118\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers,\n\u001b[0;32m    119\u001b[0m         extra_query\u001b[39m=\u001b[39;49mextra_query,\n\u001b[0;32m    120\u001b[0m         extra_body\u001b[39m=\u001b[39;49mextra_body,\n\u001b[0;32m    121\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    122\u001b[0m         post_parser\u001b[39m=\u001b[39;49mparser,\n\u001b[0;32m    123\u001b[0m     ),\n\u001b[0;32m    124\u001b[0m     cast_to\u001b[39m=\u001b[39;49mCreateEmbeddingResponse,\n\u001b[0;32m    125\u001b[0m )\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\openai\\_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1254\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1261\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1262\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1263\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1264\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1265\u001b[0m     )\n\u001b[1;32m-> 1266\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\openai\\_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    934\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    940\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    941\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 942\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    943\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    944\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    945\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    946\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    947\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    948\u001b[0m     )\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\openai\\_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m   1030\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[1;32m-> 1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m   1032\u001b[0m         input_options,\n\u001b[0;32m   1033\u001b[0m         cast_to,\n\u001b[0;32m   1034\u001b[0m         retries,\n\u001b[0;32m   1035\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m   1036\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m   1037\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1040\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\openai\\_base_client.py:1079\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1079\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m   1080\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m   1081\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m   1082\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m   1083\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m   1084\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m   1085\u001b[0m )\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\openai\\_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m   1030\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[1;32m-> 1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m   1032\u001b[0m         input_options,\n\u001b[0;32m   1033\u001b[0m         cast_to,\n\u001b[0;32m   1034\u001b[0m         retries,\n\u001b[0;32m   1035\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m   1036\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m   1037\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1040\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\openai\\_base_client.py:1079\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1079\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m   1080\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m   1081\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m   1082\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m   1083\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m   1084\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m   1085\u001b[0m )\n",
                        "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\openai\\_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m   1045\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1046\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[0;32m   1049\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[0;32m   1050\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[0;32m   1054\u001b[0m )\n",
                        "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
                    ]
                }
            ],
            "source": [
                "'''Can't use OPENAI: RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, \n",
                "please check your plan and billing details. For more information on this error, \n",
                "read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
                "'''\n",
                "# unified_df['ada_embedding'] = unified_df.combined.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
                "# unified_df.to_csv('../data/processed/embedded_movies.csv', index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tags column exported to tags.json\n"
                    ]
                }
            ],
            "source": [
                "# Select only the 'tags' column\n",
                "tags_df = unified_df[['id', 'title', 'tags']]\n",
                "\n",
                "# Export to JSON\n",
                "tags_df.to_json('tags.json', orient='records', lines=True)\n",
                "\n",
                "print(\"Tags column exported to tags.json\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "26626fbe4ff2401795c2e87319780880",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f513f0d89a2a42639e18c7d564e4b6cc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c39f6153c6074df0a706b949a4dd78da",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3996718a9d884c2fbaf614b878fdf7a0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "470627e7fd264024823a991fbd363556",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b7b6800d26104042a8f09881dbb3cae9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Embeddings of size 1536 have been generated and saved to ../data/processed/embeddings.json.\n"
                    ]
                }
            ],
            "source": [
                "# Load JSON data\n",
                "input_file = '../data/interim/tags.json'\n",
                "output_file = '../data/processed/embeddings.json'\n",
                "\n",
                "with open(input_file, 'r') as f:\n",
                "    data = [json.loads(line) for line in f]\n",
                "\n",
                "# Initialize the tokenizer and model\n",
                "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
                "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
                "\n",
                "# Function to generate embeddings\n",
                "def generate_embedding(text, tokenizer, model):\n",
                "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
                "\n",
                "# Generate embeddings for each entry\n",
                "embeddings_data = []\n",
                "for entry in data:\n",
                "    text = entry['title'] + \" \" + entry['tags']\n",
                "    embedding = generate_embedding(text, tokenizer, model)\n",
                "    entry['embedding'] = embedding.tolist()  # Convert numpy array to list for JSON serialization\n",
                "    embeddings_data.append(entry)\n",
                "\n",
                "# Save embeddings to new file\n",
                "with open(output_file, 'w') as f:\n",
                "    for entry in embeddings_data:\n",
                "        json.dump(entry, f)\n",
                "        f.write('\\n')\n",
                "\n",
                "print(f\"Embeddings of size 1536 have been generated and saved to {output_file}.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the embeddings from the JSON file\n",
                "embeddings_file = '../data/processed/embeddings.json'\n",
                "\n",
                "with open(embeddings_file, 'r') as f:\n",
                "    data = [json.loads(line) for line in f]\n",
                "\n",
                "# Create a dictionary to map movie titles to embeddings\n",
                "embeddings_dict = {entry['title']: np.array(entry['embedding']) for entry in data}\n",
                "\n",
                "# Function to find similar movies based on cosine similarity\n",
                "def find_similar_movies(movie_title, embeddings_dict, top_n=5):\n",
                "    if movie_title not in embeddings_dict:\n",
                "        return f\"Movie title '{movie_title}' not found in the dataset.\"\n",
                "\n",
                "    movie_embedding = embeddings_dict[movie_title].reshape(1, -1)\n",
                "    similarities = []\n",
                "\n",
                "    for title, embedding in embeddings_dict.items():\n",
                "        if title != movie_title:\n",
                "            sim = cosine_similarity(movie_embedding, embedding.reshape(1, -1)).flatten()[0]\n",
                "            similarities.append((title, sim))\n",
                "\n",
                "    # Sort by similarity score\n",
                "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
                "\n",
                "    return similarities[:top_n]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top 5 movies similar to 'Mission: Impossible':\n",
                        "Mission: Impossible II (Similarity: 0.8074)\n",
                        "Mission: Impossible III (Similarity: 0.8051)\n",
                        "Mission: Impossible - Rogue Nation (Similarity: 0.7233)\n",
                        "Mission: Impossible - Ghost Protocol (Similarity: 0.7065)\n",
                        "The Sentinel (Similarity: 0.5434)\n"
                    ]
                }
            ],
            "source": [
                "# Example usage\n",
                "movie_title = \"Mission: Impossible\"\n",
                "similar_movies = find_similar_movies(movie_title, embeddings_dict)\n",
                "\n",
                "print(f\"Top 5 movies similar to '{movie_title}':\")\n",
                "for title, score in similar_movies:\n",
                "    print(f\"{title} (Similarity: {score:.4f})\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tf_gpu",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.19"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "b1014857e5f514609e0bfeb89ffc23db6865fa5617010c6981bd698468837496"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
